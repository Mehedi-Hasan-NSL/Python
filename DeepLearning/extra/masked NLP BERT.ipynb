{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a1cf4e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T09:35:01.840245Z",
     "start_time": "2021-09-17T09:15:38.675097Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping keras as it is not installed.\n",
      "WARNING: Skipping keras-nightly as it is not installed.\n",
      "WARNING: Skipping keras-Preprocessing as it is not installed.\n",
      "WARNING: Skipping keras-vis as it is not installed.\n",
      "WARNING: Skipping tensorflow as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.3.0\n",
      "  Downloading tensorflow-2.3.0-cp38-cp38-win_amd64.whl (342.5 MB)\n",
      "Requirement already satisfied: astunparse==1.6.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow==2.3.0) (1.6.3)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\DELL\\\\Anaconda3\\\\Lib\\\\site-packages\\\\numpy\\\\~=ibs\\\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting numpy<1.19.0,>=1.16.0\n",
      "  Downloading numpy-1.18.5-cp38-cp38-win_amd64.whl (12.8 MB)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow==2.3.0) (0.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow==2.3.0) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow==2.3.0) (3.17.2)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Using cached tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "Collecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow==2.3.0) (1.36.1)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow==2.3.0) (0.36.2)\n",
      "Collecting scipy==1.4.1\n",
      "  Downloading scipy-1.4.1-cp38-cp38-win_amd64.whl (31.0 MB)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow==2.3.0) (0.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow==2.3.0) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow==2.3.0) (2.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow==2.3.0) (3.3.0)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow==2.3.0) (2.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow==2.3.0) (1.12.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.3.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.6.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.25.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.33.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.4)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.26.6)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2021.5.30)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.1.1)\n",
      "Installing collected packages: numpy, tensorflow-estimator, scipy, keras-preprocessing, gast, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall keras -y\n",
    "# !pip uninstall keras-nightly -y\n",
    "# !pip uninstall keras-Preprocessing -y\n",
    "# !pip uninstall keras-vis -y\n",
    "# !pip uninstall tensorflow -y\n",
    "\n",
    "# !pip install tensorflow==2.3.0\n",
    "# !pip install keras==2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ede7d236",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T18:13:24.036463Z",
     "start_time": "2021-09-18T18:13:24.012808Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0184bf39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T18:13:51.688142Z",
     "start_time": "2021-09-18T18:13:51.642622Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    MAX_LEN = 256\n",
    "    BATCH_SIZE = 32\n",
    "    LR = 0.001\n",
    "    VOCAB_SIZE = 30000\n",
    "    EMBED_DIM = 128\n",
    "    NUM_HEAD = 8  # used in bert model\n",
    "    FF_DIM = 128  # used in bert model\n",
    "    NUM_LAYERS = 1\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "80aed692",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T19:25:22.344767Z",
     "start_time": "2021-09-18T18:13:57.518085Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0 80.2M    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  0 80.2M    0 32768    0     0   9586      0  2:26:15  0:00:03  2:26:12  9586\n",
      "  0 80.2M    0 32768    0     0   7401      0  3:09:26  0:00:04  3:09:22  7403\n",
      "  0 80.2M    0 32768    0     0   6809      0  3:25:55  0:00:04  3:25:51  6809\n",
      "  0 80.2M    0 32768    0     0   5045      0  4:37:55  0:00:06  4:37:49  5733\n",
      "  0 80.2M    0 32768    0     0   4363      0  5:21:21  0:00:07  5:21:14  5853\n",
      "  0 80.2M    0 32768    0     0   4070      0  5:44:29  0:00:08  5:44:21     0\n",
      "  0 80.2M    0 49152    0     0   5292      0  4:24:56  0:00:09  4:24:47  3370\n",
      "  0 80.2M    0 49152    0     0   4777      0  4:53:30  0:00:10  4:53:20  2991\n",
      "  0 80.2M    0 49152    0     0   4349      0  5:22:23  0:00:11  5:22:12  3408\n",
      "  0 80.2M    0 49152    0     0   3992      0  5:51:13  0:00:12  5:51:01  3411\n",
      "  0 80.2M    0 49152    0     0   3688      0  6:20:10  0:00:13  6:19:57  3105\n",
      "  0 80.2M    0 49152    0     0   3427      0  6:49:07  0:00:14  6:48:53     0\n",
      "  0 80.2M    0 49152    0     0   3201      0  7:18:01  0:00:15  7:17:46     0\n",
      "  0 80.2M    0 49152    0     0   3003      0  7:46:53  0:00:16  7:46:37     0\n",
      "  0 80.2M    0 49152    0     0   2828      0  8:15:47  0:00:17  8:15:30     0\n",
      "  0 80.2M    0 49152    0     0   2671      0  8:44:56  0:00:18  8:44:38     0\n",
      "  0 80.2M    0 49152    0     0   2532      0  9:13:45  0:00:19  9:13:26     0\n",
      "  0 80.2M    0 49152    0     0   2408      0  9:42:15  0:00:20  9:41:55     0\n",
      "  0 80.2M    0 49152    0     0   2294      0 10:11:12  0:00:21 10:10:51     0\n",
      "  0 80.2M    0 49152    0     0   2190      0 10:40:13  0:00:22 10:39:51     0\n",
      "  0 80.2M    0 49152    0     0   2132      0 10:57:38  0:00:23 10:57:15     0\n",
      "  0 80.2M    0 65536    0     0   2762      0  8:27:38  0:00:23  8:27:15  3796\n",
      "  0 80.2M    0 81920    0     0   3294      0  7:05:39  0:00:24  7:05:15  7361\n",
      "  0 80.2M    0  112k    0     0   4449      0  5:15:08  0:00:25  5:14:43 15048\n",
      "  0 80.2M    0  160k    0     0   6089      0  3:50:16  0:00:26  3:49:50 25663\n",
      "  0 80.2M    0  208k    0     0   7658      0  3:03:05  0:00:27  3:02:38 34384\n",
      "  0 80.2M    0  256k    0     0   9127      0  2:33:37  0:00:28  2:33:09 39360\n",
      "  0 80.2M    0  304k    0     0  10486      0  2:13:42  0:00:29  2:13:13 47548\n",
      "  0 80.2M    0  336k    0     0  11185      0  2:05:21  0:00:30  2:04:51 46040\n",
      "  0 80.2M    0  368k    0     0  11860      0  1:58:13  0:00:31  1:57:42 43762\n",
      "  0 80.2M    0  416k    0     0  12901      0  1:48:40  0:00:33  1:48:07 40912\n",
      "  0 80.2M    0  432k    0     0  13041      0  1:47:30  0:00:33  1:46:57 34651\n",
      "  0 80.2M    0  432k    0     0  12671      0  1:50:39  0:00:34  1:50:05 25085\n",
      "  0 80.2M    0  448k    0     0  12782      0  1:49:41  0:00:35  1:49:06 22365\n",
      "  0 80.2M    0  464k    0     0  12759      0  1:49:53  0:00:37  1:49:16 17987\n",
      "  0 80.2M    0  464k    0     0  12423      0  1:52:51  0:00:38  1:52:13  9401\n",
      "  0 80.2M    0  464k    0     0  12204      0  1:54:53  0:00:38  1:54:15  6540\n",
      "  0 80.2M    0  480k    0     0  12323      0  1:53:46  0:00:39  1:53:07  9879\n",
      "  0 80.2M    0  496k    0     0  12438      0  1:52:43  0:00:40  1:52:03  9939\n",
      "  0 80.2M    0  496k    0     0  12151      0  1:55:23  0:00:41  1:54:42  7189\n",
      "  0 80.2M    0  512k    0     0  12248      0  1:54:28  0:00:42  1:53:46 10781\n",
      "  0 80.2M    0  544k    0     0  12652      0  1:50:49  0:00:44  1:50:05 16072\n",
      "  0 80.2M    0  560k    0     0  12816      0  1:49:24  0:00:44  1:48:40 16866\n",
      "  0 80.2M    0  592k    0     0  13214      0  1:46:06  0:00:45  1:45:21 19497\n",
      "  0 80.2M    0  608k    0     0  13309      0  1:45:20  0:00:46  1:44:34 23011\n",
      "  0 80.2M    0  640k    0     0  13677      0  1:42:30  0:00:47  1:41:43 25650\n",
      "  0 80.2M    0  656k    0     0  13758      0  1:41:54  0:00:48  1:41:06 23923\n",
      "  0 80.2M    0  672k    0     0  13774      0  1:41:47  0:00:49  1:40:58 22000\n",
      "  0 80.2M    0  688k    0     0  13900      0  1:40:52  0:00:50  1:40:02 20441\n",
      "  0 80.2M    0  688k    0     0  13527      0  1:43:39  0:00:52  1:42:47 15459\n",
      "  0 80.2M    0  704k    0     0  13664      0  1:42:36  0:00:52  1:41:44 13537\n",
      "  0 80.2M    0  720k    0     0  13730      0  1:42:07  0:00:53  1:41:14 13440\n",
      "  0 80.2M    0  736k    0     0  13767      0  1:41:50  0:00:54  1:40:56 13693\n",
      "  0 80.2M    0  752k    0     0  13814      0  1:41:29  0:00:55  1:40:34 12951\n",
      "  0 80.2M    0  752k    0     0  13569      0  1:43:19  0:00:56  1:42:23 14033\n",
      "  0 80.2M    0  768k    0     0  13613      0  1:42:59  0:00:57  1:42:02 13070\n",
      "  0 80.2M    0  768k    0     0  13290      0  1:45:30  0:00:59  1:44:31  8980\n",
      "  0 80.2M    0  768k    0     0  13068      0  1:47:17  0:01:00  1:46:17  6030\n",
      "  0 80.2M    0  768k    0     0  12923      0  1:48:29  0:01:00  1:47:29  3207\n",
      "  0 80.2M    0  784k    0     0  12996      0  1:47:53  0:01:01  1:46:52  6520\n",
      "  0 80.2M    0  816k    0     0  13199      0  1:46:13  0:01:03  1:45:10  8878\n",
      "  0 80.2M    0  816k    0     0  13105      0  1:46:59  0:01:03  1:45:56 10710\n",
      "  1 80.2M    1  832k    0     0  13152      0  1:46:36  0:01:04  1:45:32 14243\n",
      "  1 80.2M    1  832k    0     0  12926      0  1:48:28  0:01:05  1:47:23 12967\n",
      "  1 80.2M    1  848k    0     0  12960      0  1:48:11  0:01:07  1:47:04 12542\n",
      "  1 80.2M    1  864k    0     0  13054      0  1:47:24  0:01:07  1:46:17 11000\n",
      "  1 80.2M    1  880k    0     0  13054      0  1:47:24  0:01:09  1:46:15 12445\n",
      "  1 80.2M    1  880k    0     0  12784      0  1:49:40  0:01:10  1:48:30  8609\n",
      "  1 80.2M    1  896k    0     0  12945      0  1:48:18  0:01:10  1:47:08 13199\n",
      "  1 80.2M    1  896k    0     0  12755      0  1:49:55  0:01:11  1:48:44  9969\n",
      "  1 80.2M    1  896k    0     0  12577      0  1:51:28  0:01:12  1:50:16  6334\n",
      "  1 80.2M    1  928k    0     0  12828      0  1:49:17  0:01:14  1:48:03  9733\n",
      "  1 80.2M    1  928k    0     0  12672      0  1:50:38  0:01:14  1:49:24 10925\n",
      "  1 80.2M    1  944k    0     0  12767      0  1:49:49  0:01:15  1:48:34 10159\n",
      "  1 80.2M    1  960k    0     0  12735      0  1:50:05  0:01:17  1:48:48 12464\n",
      "  1 80.2M    1  976k    0     0  12763      0  1:49:51  0:01:18  1:48:33 15286\n",
      "  1 80.2M    1  976k    0     0  12689      0  1:50:29  0:01:18  1:49:11 10498\n",
      "  1 80.2M    1  992k    0     0  12707      0  1:50:20  0:01:19  1:49:01 13231\n",
      "  1 80.2M    1  992k    0     0  12553      0  1:51:41  0:01:20  1:50:21  9435\n",
      "  1 80.2M    1 1008k    0     0  12636      0  1:50:57  0:01:21  1:49:36 10925\n",
      "  1 80.2M    1 1024k    0     0  12679      0  1:50:35  0:01:22  1:49:13 11191\n",
      "  1 80.2M    1 1024k    0     0  12492      0  1:52:14  0:01:23  1:50:51  9497\n",
      "  1 80.2M    1 1024k    0     0  12323      0  1:53:46  0:01:25  1:52:21  6362\n",
      "  1 80.2M    1 1024k    0     0  12179      0  1:55:07  0:01:26  1:53:41  6335\n",
      "  1 80.2M    1 1024k    0     0  12038      0  1:56:28  0:01:27  1:55:01  3023\n",
      "  1 80.2M    1 1040k    0     0  12059      0  1:56:16  0:01:28  1:54:48  2921\n",
      "  1 80.2M    1 1040k    0     0  11923      0  1:57:35  0:01:29  1:56:06  3045\n",
      "  1 80.2M    1 1040k    0     0  11790      0  1:58:55  0:01:30  1:57:25  3130\n",
      "  1 80.2M    1 1040k    0     0  11661      0  2:00:14  0:01:31  1:58:43  3130\n",
      "  1 80.2M    1 1056k    0     0  11766      0  1:59:09  0:01:31  1:57:38  6833\n",
      "  1 80.2M    1 1072k    0     0  11831      0  1:58:30  0:01:32  1:56:58  7322\n",
      "  1 80.2M    1 1088k    0     0  11836      0  1:58:27  0:01:34  1:56:53 10210\n",
      "  1 80.2M    1 1104k    0     0  11913      0  1:57:41  0:01:34  1:56:07 14327\n",
      "  1 80.2M    1 1120k    0     0  11939      0  1:57:26  0:01:36  1:55:50 17300\n",
      "  1 80.2M    1 1120k    0     0  11848      0  1:58:20  0:01:36  1:56:44 13371\n",
      "  1 80.2M    1 1136k    0     0  11872      0  1:58:06  0:01:37  1:56:29 12612\n",
      "  1 80.2M    1 1136k    0     0  11759      0  1:59:14  0:01:38  1:57:36 10254\n",
      "  1 80.2M    1 1152k    0     0  11791      0  1:58:54  0:01:40  1:57:14  9544\n",
      "  1 80.2M    1 1168k    0     0  11868      0  1:58:08  0:01:40  1:56:28 10424\n",
      "  1 80.2M    1 1168k    0     0  11703      0  1:59:48  0:01:42  1:58:06  9117\n",
      "  1 80.2M    1 1184k    0     0  11785      0  1:58:58  0:01:42  1:57:16 10028\n",
      "  1 80.2M    1 1184k    0     0  11670      0  2:00:08  0:01:43  1:58:25  9905\n",
      "  1 80.2M    1 1200k    0     0  11729      0  1:59:32  0:01:44  1:57:48 10413\n",
      "  1 80.2M    1 1232k    0     0  11926      0  1:57:33  0:01:45  1:55:48 13101\n",
      "  1 80.2M    1 1232k    0     0  11796      0  1:58:51  0:01:46  1:57:05 13794\n",
      "  1 80.2M    1 1248k    0     0  11847      0  1:58:21  0:01:47  1:56:34 13128\n",
      "  1 80.2M    1 1280k    0     0  12049      0  1:56:21  0:01:48  1:54:33 20078\n",
      "  1 80.2M    1 1280k    0     0  11946      0  1:57:22  0:01:49  1:55:33 16529\n",
      "  1 80.2M    1 1296k    0     0  11989      0  1:56:56  0:01:50  1:55:06 13328\n",
      "  1 80.2M    1 1328k    0     0  12112      0  1:55:45  0:01:52  1:53:53 18460\n",
      "  1 80.2M    1 1328k    0     0  12059      0  1:56:16  0:01:52  1:54:24 16738\n",
      "  1 80.2M    1 1344k    0     0  12022      0  1:56:37  0:01:54  1:54:43 11519\n",
      "  1 80.2M    1 1344k    0     0  11918      0  1:57:38  0:01:55  1:55:43 11393\n",
      "  1 80.2M    1 1344k    0     0  11815      0  1:58:40  0:01:56  1:56:44  8499\n",
      "  1 80.2M    1 1344k    0     0  11714      0  1:59:41  0:01:57  1:57:44  3139\n",
      "  1 80.2M    1 1344k    0     0  11613      0  2:00:44  0:01:58  1:58:46  2856\n",
      "  1 80.2M    1 1344k    0     0  11516      0  2:01:45  0:01:59  1:59:46     0\n",
      "  1 80.2M    1 1344k    0     0  11433      0  2:02:38  0:02:00  2:00:38     0\n",
      "  1 80.2M    1 1360k    0     0  11526      0  2:01:38  0:02:00  1:59:38  3774\n",
      "  1 80.2M    1 1360k    0     0  11409      0  2:02:53  0:02:02  2:00:51  3578\n",
      "  1 80.2M    1 1360k    0     0  11318      0  2:03:52  0:02:03  2:01:49  3606\n",
      "  1 80.2M    1 1376k    0     0    328      0 71:14:41  1:11:23 70:03:18     7\n",
      "curl: (56) Send failure: Connection was reset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb/test/labeledBow.feat: truncated gzip input\n",
      "tar: Error exit delayed from previous errors.\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9ece63a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T06:01:28.814891Z",
     "start_time": "2021-09-17T06:01:28.687082Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb/test/labeledBow.feat: truncated gzip input\n",
      "tar: Error exit delayed from previous errors.\n"
     ]
    }
   ],
   "source": [
    "#tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "30d8b0a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T14:09:37.295159Z",
     "start_time": "2021-09-18T14:09:37.288504Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_text_list_from_files(files):\n",
    "    text_list = []\n",
    "    for name in files:\n",
    "        with open(name,'rb') as f:\n",
    "            for line in f:\n",
    "                text_list.append(line)\n",
    "    return text_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7016be55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T14:09:38.937942Z",
     "start_time": "2021-09-18T14:09:38.916959Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data_from_text_files(folder_name):\n",
    "\n",
    "    pos_files = glob.glob(\"aclImdb/\" + folder_name + \"/pos/*.txt\")\n",
    "    pos_texts = get_text_list_from_files(pos_files)\n",
    "    neg_files = glob.glob(\"aclImdb/\" + folder_name + \"/neg/*.txt\")\n",
    "    neg_texts = get_text_list_from_files(neg_files)\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"review\": pos_texts + neg_texts,\n",
    "            \"sentiment\": [0] * len(pos_texts) + [1] * len(neg_texts),\n",
    "        }\n",
    "    )\n",
    "    df = df.sample(len(df)).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee5671f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-18T14:09:41.824Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = get_data_from_text_files(\"train\")\n",
    "test_df = get_data_from_text_files(\"test\")\n",
    "\n",
    "all_data = train_df.append(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9c317b1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T13:18:35.748342Z",
     "start_time": "2021-09-18T13:18:35.731022Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"), \"\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "140838f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T13:58:14.931245Z",
     "start_time": "2021-09-18T13:58:14.894816Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n",
    "    \"\"\"Build Text vectorization layer\n",
    "\n",
    "    Args:\n",
    "      texts (list): List of string i.e input texts\n",
    "      vocab_size (int): vocab size\n",
    "      max_seq (int): Maximum sequence lenght.\n",
    "      special_tokens (list, optional): List of special tokens. Defaults to ['[MASK]'].\n",
    "\n",
    "    Returns:\n",
    "        layers.Layer: Return TextVectorization Keras Layer\n",
    "    \"\"\"\n",
    "    vectorize_layer = TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode=\"int\",\n",
    "        standardize=custom_standardization,\n",
    "        output_sequence_length=max_seq,\n",
    "    )\n",
    "    vectorize_layer.adapt(texts)\n",
    "\n",
    "    \n",
    "    # Insert mask token in vocabulary\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "    vocab = vocab[2 : vocab_size - len(special_tokens)] + [\"[mask]\"]\n",
    "    vectorize_layer.set_vocabulary(vocab)\n",
    "    return vectorize_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dede3bd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T13:08:28.919472Z",
     "start_time": "2021-09-18T13:08:28.894841Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['or: It\\'s a bird ? It\\'s a plane ? No, look... It\\'s a disaster ! or: No need to look up in the sky.<br /><br />or: (... OK, that\\'s enough.) If singer tried to make a romantic titanic like movie to crash the box office record, he failed. The SR structure can\\'t do this, the dark and restricted color scheme (I would call it \"wishi-washi\"), the boring usual dialogs, the clown with advanced alien technology, the missing fun and magic, etc. makes the movie completely disappointing.<br /><br />It simply doesn\\'t work.<br /><br />The main thing at a character like superman is, that he is a superhero. That\\'s the core, the most important thing.<br /><br />Love trouble and a sadly lost depressive Supersoftie can maximal only be a facet in a 2:40 long Superman movie, not the whole time.<br /><br />Because then it is not a superman or superhero movie anymore. It\\'s like a (and in this case a very bad and boring) social study, where every 30 minutes a person flies around.<br /><br />That\\'s a big difference.<br /><br />This movie is a joke. Holy skywalker, this is Superman, so give us Supervillains, Superaction and the most important thing, Superfun.<br /><br />We want ENTERTAINMENT ! Singer, if you want to make a 2:40 long soul love trouble drama about lost and sadly people ? Then take normal human characters and make a yentl remake.<br /><br />They say: Superman returns. And then, supersadlysoftie stands in the door.<br /><br />Maybe singer has tried to kill the legend without kryptonite, but one bad movie can\\'t do this. Don\\'t give up, they plan a sequel...<br /><br />Next time singer (and Warner Brothers), make a superhero movie, not a depressive superlame soap, or let it be. A superhero movie means a colorful fantasy with a lot of fun and magic.<br /><br />This movie is like a very cheap chocolate box with a super cover. Yes, technically there are all sorts in, yes, there is a lot of small talks, there are a few jokes, a view action scenes, etc, but the only one what all these worse pieces have in common is their poor quality.<br /><br />It\\'s not more than a super boring patchwork and one of the worst movies I have ever seen in my live.<br /><br />The ridiculous cast strategy (Cast them young as possible, so we can make sequels in the next 25 years) gives the rest. Kate Bosworth plays a 22 year old star reporter, she\\'s looking like 19. Superman was full five years away, so he slept with her 6 years ago, so she was 16, (looking like 13) and a daily planet reporter, wroting \"I spend a night with superman\". Warner Brothers, that\\'s too much.<br /><br />Routh is not so bad, he is playing a little bit wooden, but the whole movie is wooden, so... ?! Temporary good were marsden and sometimes posey. The rest, forget it. Even spacey, this is not his terrain.<br /><br />Reeve/Hackman/Kidder were acting so easy, with fun. What a difference.<br /><br />The Jesus poses at the end are ridiculous too. What the hell should that be ? The problem here is, they mean this serious, not as a joke. Next time Spiderman or Batman or Ironman falls and rises like Jesus or angels ? Or they speak with god directly. Why not ? They are superheroes, saving human lives every day. So at least one talk with god every week should be possible... Oh my god.<br /><br />Maybe this was not the real announced movie, instead it is from a bizarre dull parallel universe.<br /><br />For the warner brothers this superlame depressive flick will be possibly the greatest disaster in history. Not only because of the money.<br /><br />I understand how difficult it (maybe) was to create and transport some messages or feelings, but showing lone, lost and sadly people isn\\'t new and thousands of movies or TV-Shows did it better, in very old or new ones like magnolia. And the flying frogs there were more impressive than this flying superwoman, sorry, superman of course.<br /><br />Singer and WB, that\\'s simply nothing. In fact it\\'s even more than nothing, it\\'s like a black hole that destroys the passion for (comic/superhero) movies and steals us three hours of our life.<br /><br />Mrs. Smilla\\'s little brother. (Very angry and green like the hulk.)',\n",
       "       'No wonder most of the cast wished they never made this movie. It\\'s just plain ridiculous and embarrassing to watch. Bad actors reading cheesy lines while shiny classic showroom cars continuously circle a diner that looks more like a Disneyland attraction. Students fist-fight with the deranged principal as he tries to stop them from setting fire to a bronze civil war statue. The Watts riots with a cast of...ugh...10?? Dermot Mulroney tries not to gag while he makes out with a Mary Hartman look-alike with the most annoying smile since \\'Mr. Sardonicus\\'. Noah Wyle reads Bob Dylan lyrics to the wicked teacher with a swinging pointer and very bad face lift. Drunken virgin Rick Schroder sits in a kiddie rocket on his last night before entering the service. Silly, giggling school girls dress up in leopard stretch pants and walk on the set of \\'Shindig\\', sing horribly off key, and actually make it big in the music business. And who wrote this compelling dialog?: \"I\\'m going to Burkley and wear flowers in my hair\"....\"I think I found someone to buy Stick\\'s woody!\"....\"These people are \\'animals\\'!\" \"These people are my \\'family\\'! as the Shirelles sing \"Mama Said\". Oh brother, What a mess. This is like a \\'Reefer Madness\\' of the 60\\'s except it\\'s not even funny.',\n",
       "       'The opening sequence alone is worth the cost of admission, as Cheech and Chong drag that big ol garbage can across the parking lot, filled with gas. \"Don\\'t Spill it Man !\", hilarious stuff. And then, as \\'the plot\\' ensues, you\\'re in for one heck of a ride. I watched this film recently and it holds up, being just as funny upon each viewing. check it out.',\n",
       "       ...,\n",
       "       'CONTAINS \"SPOILER\" INFORMATION. Watch this director\\'s other film, \"Earth\", at some point. It\\'s a better film, but this one isn\\'t bad just different.<br /><br />A rare feminist point of view from an Indian filmmaker. Tradition, rituals, duty, secrets, and the portrayal of strict sex roles make this an engaging and culturally dynamic film viewing experience. All of the married characters lack the \"fire\" of the marriage bed with their respective spouses. One husband is celibate and commits a form of spiritual \"adultery\" by giving all of his love, honor, time and respect to his religious swami (guru). His wife is lonely and yearns for intimacy and tenderness which she eventually finds with her closeted lesbian sister-in-law who comes to live in their house with her unfaithful husband. This unfaithful husband is openly in love with his Chinese mistress but was forced into marriage with a (unbeknownest to him) lesbian. They only have sex once when his closet lesbian wife loses her virginity.<br /><br />A servant lives in the house and he eventually reveals the secret that the two women are lovers. Another significant character is the elderly matriarch who is unable to speak or care for herself due to a stroke. However, she uses a ringing bell to communicate her needs as well as her displeasure with the family members. She lets them know through her bell or by pounding her fist that she knows exacly what\\'s going on in the house and how much she disapproves.<br /><br />In the end, the truth about everybody comes out and the two female lovers end up running away together. But, not before there is an emotional scene between the swami-addicted husband and his formerly straight wife. Her sari catches on fire and at first we think she is going to die. However, we see the two women united in the very last scene of the movie.<br /><br />The writer/director of this film challenges her culture\\'s traditions, but she shows us individual human beings who are trapped by their culture and gender. We come to really care about the characters and we don\\'t see them as stereotypes. Each on surprises us with their humanity, vulgarity, tenderness, anger, and spirit.',\n",
       "       'Yet another in the long line of \"Don\\'t\" films of the late 70\\'s and early 80\\'s yet this one is much more than that. This film is a highly underestimated low budget schlocker with a twist. It has the grainy quality and bizarre soundtrack that is typical of horror films of the time period but it\\'s the highly underestimated performances of the surprisingly talented actors/actresses that make this movie good. A young nurse arrives at Dr. Stephens\\' progressive mental hospital right after he has been murdered by one of his patients and all is not what it appears to be. It seems Dr. Masters, a rather ambitious female doctor, has taken over his duties and begun to implement her own ideas. Each of the patients take on their own unique personalities and have their own personality traits and flaws which make for highly entertaining interactions. There is the nymphomaniac, the crazy old crone, the woman with an unhealthy obsession with infants, and a man who has reverted back to his childhood among others. There is also a strange little twist to this bizarre story that later finds the young nurse trapped inside the asylum with the patients running around loose and bodies piling up. If you are a fan of cheap 70\\'s sleaze than this is the film for you!',\n",
       "       'This movie, \"Desperate Measures\", was.... I\\'m not quite sure how to even put it into words. Was this supposed to be a comedy? I couldn\\'t stop laughing at how absolutely ridiculous it was. I love Michael Keaton, and I cannot actually comprehend that he did this. They did a good job at keeping my attention because I couldn\\'t wait to see how much more ridiculous it was going to get minute by minute. I actually just registered on this site so that I could get this out. I don\\'t review movies. I don\\'t have time for this, but I cannot let this go knowing I haven\\'t done my civil duty by letting people (those who have an IQ of 85 and above) know that this is no action/thriller, It is honest to goodness funny. You people that actually got thrills off of this scare me. Go watch a good movie like Million Dollar Baby. Clint Eastwood\\'s acting is not superb, but I was balling at the end. Exercise your brain America!'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.review.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7ac7d1dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T13:58:45.870736Z",
     "start_time": "2021-09-18T13:58:26.108919Z"
    }
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc2 in position 2: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2088/2859985247.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m vectorize_layer = get_vectorize_layer(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mall_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mspecial_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"[mask]\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2088/1342554783.py\u001b[0m in \u001b[0;36mget_vectorize_layer\u001b[1;34m(texts, vocab_size, max_seq, special_tokens)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# Insert mask token in vocabulary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorize_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspecial_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"[mask]\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mvectorize_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp2\\lib\\site-packages\\keras\\layers\\preprocessing\\text_vectorization.py\u001b[0m in \u001b[0;36mget_vocabulary\u001b[1;34m(self, include_special_tokens)\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[0mvocabulary\u001b[0m \u001b[0mwill\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0minclude\u001b[0m \u001b[0many\u001b[0m \u001b[0mpadding\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mOOV\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \"\"\"\n\u001b[1;32m--> 355\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_lookup_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minclude_special_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mvocabulary_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp2\\lib\\site-packages\\keras\\layers\\preprocessing\\index_lookup.py\u001b[0m in \u001b[0;36mget_vocabulary\u001b[1;34m(self, include_special_tokens)\u001b[0m\n\u001b[0;32m    300\u001b[0m       \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlookup_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m       \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minvert\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensor_vocab_to_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m     lookup = collections.defaultdict(lambda: self.oov_token,\n\u001b[0;32m    304\u001b[0m                                      zip(indices, vocab))\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp2\\lib\\site-packages\\keras\\layers\\preprocessing\\string_lookup.py\u001b[0m in \u001b[0;36m_tensor_vocab_to_numpy\u001b[1;34m(self, vocabulary)\u001b[0m\n\u001b[0;32m    334\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_tensor_vocab_to_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp2\\lib\\site-packages\\keras\\layers\\preprocessing\\string_lookup.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    334\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_tensor_vocab_to_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp2\\lib\\site-packages\\tensorflow\\python\\util\\compat.py\u001b[0m in \u001b[0;36mas_text\u001b[1;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbytes_or_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes_or_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbytes_or_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Expected binary or unicode string, got %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbytes_or_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc2 in position 2: unexpected end of data"
     ]
    }
   ],
   "source": [
    "vectorize_layer = get_vectorize_layer(\n",
    "    all_data.review.values.tolist(),\n",
    "    config.VOCAB_SIZE,\n",
    "    config.MAX_LEN,\n",
    "    special_tokens=[\"[mask]\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40777bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a95755",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:45:06.789791Z",
     "start_time": "2021-09-17T10:45:06.789791Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get mask token id for masked language model\n",
    "mask_token_id = vectorize_layer([\"[mask]\"]).numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3be265c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:45:06.797104Z",
     "start_time": "2021-09-17T10:45:06.797104Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode(texts):\n",
    "    encoded_texts = vectorize_layer(texts)\n",
    "    return encoded_texts.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d754f67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:45:06.803411Z",
     "start_time": "2021-09-17T10:45:06.803411Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_masked_input_and_labels(encoded_texts):\n",
    "    # 15% BERT masking\n",
    "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n",
    "    # Do not mask special tokens\n",
    "    inp_mask[encoded_texts <= 2] = False\n",
    "    # Set targets to -1 by default, it means ignore\n",
    "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
    "    # Set labels for masked tokens\n",
    "    labels[inp_mask] = encoded_texts[inp_mask]\n",
    "\n",
    "    # Prepare input\n",
    "    encoded_texts_masked = np.copy(encoded_texts)\n",
    "    # Set input to [MASK] which is the last token for the 90% of tokens\n",
    "    # This means leaving 10% unchanged\n",
    "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
    "    encoded_texts_masked[\n",
    "        inp_mask_2mask\n",
    "    ] = mask_token_id  # mask token is the last in the dict\n",
    "\n",
    "    # Set 10% to a random token\n",
    "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n",
    "    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n",
    "        3, mask_token_id, inp_mask_2random.sum()\n",
    "    )\n",
    "\n",
    "    # Prepare sample_weights to pass to .fit() method\n",
    "    sample_weights = np.ones(labels.shape)\n",
    "    sample_weights[labels == -1] = 0\n",
    "\n",
    "    # y_labels would be same as encoded_texts i.e input tokens\n",
    "    y_labels = np.copy(encoded_texts)\n",
    "\n",
    "    return encoded_texts_masked, y_labels, sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ad6c8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:45:06.810270Z",
     "start_time": "2021-09-17T10:45:06.810270Z"
    }
   },
   "outputs": [],
   "source": [
    "# We have 25000 examples for training\n",
    "x_train = encode(train_df.review.values)  # encode reviews with vectorizer\n",
    "y_train = train_df.sentiment.values\n",
    "train_classifier_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    .shuffle(1000)\n",
    "    .batch(config.BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbe115e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:45:06.818922Z",
     "start_time": "2021-09-17T10:45:06.818922Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3777d512",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:45:06.826062Z",
     "start_time": "2021-09-17T10:45:06.826062Z"
    }
   },
   "outputs": [],
   "source": [
    "# We have 25000 examples for testing\n",
    "x_test = encode(test_df.review.values)\n",
    "y_test = test_df.sentiment.values\n",
    "test_classifier_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(\n",
    "    config.BATCH_SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6c4840",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:45:06.832533Z",
     "start_time": "2021-09-17T10:45:06.832533Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build dataset for end to end model input (will be used at the end)\n",
    "test_raw_classifier_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (test_df.review.values, y_test)\n",
    ").batch(config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b38cd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:45:06.838896Z",
     "start_time": "2021-09-17T10:45:06.838896Z"
    }
   },
   "outputs": [],
   "source": [
    "test_raw_classifier_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaac7f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:45:06.845470Z",
     "start_time": "2021-09-17T10:45:06.845470Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare data for masked language model\n",
    "x_all_review = encode(all_data.review.values)\n",
    "x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(\n",
    "    x_all_review\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29574074",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:45:06.852109Z",
     "start_time": "2021-09-17T10:45:06.852109Z"
    }
   },
   "outputs": [],
   "source": [
    "mlm_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_masked_train, y_masked_labels, sample_weights)\n",
    ")\n",
    "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3a687a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:45:06.858637Z",
     "start_time": "2021-09-17T10:45:06.858637Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a604022e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:45:06.864875Z",
     "start_time": "2021-09-17T10:45:06.864875Z"
    }
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(device_count = {'XLA_GPU': 0})\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3a23690d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T08:46:36.067828Z",
     "start_time": "2021-09-17T08:46:36.060299Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb5c9e51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T09:09:04.179533Z",
     "start_time": "2021-09-17T09:09:04.125236Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e6a001719c46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m loss_fn = keras.losses.SparseCategoricalCrossentropy(\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNONE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Create BERT model (Pretraining Model) for masked language modeling\n",
    "\n",
    "We will create a BERT-like pretraining model architecture\n",
    "using the `MultiHeadAttention` layer.\n",
    "It will take token ids as inputs (including masked tokens)\n",
    "and it will predict the correct ids for the masked input tokens.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def bert_module(query, key, value, i):\n",
    "    # Multi headed self-attention\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=config.NUM_HEAD,\n",
    "        key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
    "        name=\"encoder_{}/multiheadattention\".format(i),\n",
    "    )(query, key, value)\n",
    "    attention_output = layers.Dropout(0.1, name=\"encoder_{}/att_dropout\".format(i))(\n",
    "        attention_output\n",
    "    )\n",
    "    attention_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=\"encoder_{}/att_layernormalization\".format(i)\n",
    "    )(query + attention_output)\n",
    "\n",
    "    # Feed-forward layer\n",
    "    ffn = keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
    "            layers.Dense(config.EMBED_DIM),\n",
    "        ],\n",
    "        name=\"encoder_{}/ffn\".format(i),\n",
    "    )\n",
    "    ffn_output = ffn(attention_output)\n",
    "    ffn_output = layers.Dropout(0.1, name=\"encoder_{}/ffn_dropout\".format(i))(\n",
    "        ffn_output\n",
    "    )\n",
    "    sequence_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=\"encoder_{}/ffn_layernormalization\".format(i)\n",
    "    )(attention_output + ffn_output)\n",
    "    return sequence_output\n",
    "\n",
    "\n",
    "def get_pos_encoding_matrix(max_len, d_emb):\n",
    "    pos_enc = np.array(\n",
    "        [\n",
    "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n",
    "            if pos != 0\n",
    "            else np.zeros(d_emb)\n",
    "            for pos in range(max_len)\n",
    "        ]\n",
    "    )\n",
    "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
    "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
    "    return pos_enc\n",
    "\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "    reduction=tf.keras.losses.Reduction.NONE\n",
    ")\n",
    "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "\n",
    "class MaskedLanguageModel(tf.keras.Model):\n",
    "    def train_step(self, inputs):\n",
    "        if len(inputs) == 3:\n",
    "            features, labels, sample_weight = inputs\n",
    "        else:\n",
    "            features, labels = inputs\n",
    "            sample_weight = None\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(features, training=True)\n",
    "            loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Compute our own metrics\n",
    "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
    "\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {\"loss\": loss_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [loss_tracker]\n",
    "\n",
    "\n",
    "def create_masked_language_bert_model():\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
    "\n",
    "    word_embeddings = layers.Embedding(\n",
    "        config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\"\n",
    "    )(inputs)\n",
    "    position_embeddings = layers.Embedding(\n",
    "        input_dim=config.MAX_LEN,\n",
    "        output_dim=config.EMBED_DIM,\n",
    "        weights=[get_pos_encoding_matrix(config.MAX_LEN, config.EMBED_DIM)],\n",
    "        name=\"position_embedding\",\n",
    "    )(tf.range(start=0, limit=config.MAX_LEN, delta=1))\n",
    "    embeddings = word_embeddings + position_embeddings\n",
    "\n",
    "    encoder_output = embeddings\n",
    "    for i in range(config.NUM_LAYERS):\n",
    "        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n",
    "\n",
    "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(\n",
    "        encoder_output\n",
    "    )\n",
    "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    mlm_model.compile(optimizer=optimizer)\n",
    "    return mlm_model\n",
    "\n",
    "\n",
    "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
    "token2id = {y: x for x, y in id2token.items()}\n",
    "\n",
    "\n",
    "class MaskedTextGenerator(keras.callbacks.Callback):\n",
    "    def __init__(self, sample_tokens, top_k=5):\n",
    "        self.sample_tokens = sample_tokens\n",
    "        self.k = top_k\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return \" \".join([id2token[t] for t in tokens if t != 0])\n",
    "\n",
    "    def convert_ids_to_tokens(self, id):\n",
    "        return id2token[id]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        prediction = self.model.predict(self.sample_tokens)\n",
    "\n",
    "        masked_index = np.where(self.sample_tokens == mask_token_id)\n",
    "        masked_index = masked_index[1]\n",
    "        mask_prediction = prediction[0][masked_index]\n",
    "\n",
    "        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n",
    "        values = mask_prediction[0][top_indices]\n",
    "\n",
    "        for i in range(len(top_indices)):\n",
    "            p = top_indices[i]\n",
    "            v = values[i]\n",
    "            tokens = np.copy(sample_tokens[0])\n",
    "            tokens[masked_index[0]] = p\n",
    "            result = {\n",
    "                \"input_text\": self.decode(sample_tokens[0].numpy()),\n",
    "                \"prediction\": self.decode(tokens),\n",
    "                \"probability\": v,\n",
    "                \"predicted mask token\": self.convert_ids_to_tokens(p),\n",
    "            }\n",
    "            pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "787c8778",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T08:48:00.968113Z",
     "start_time": "2021-09-17T08:48:00.766737Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras.layers.experimental.preprocessing' has no attribute 'MultiHeadAttention'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-c8856b05b5be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mgenerator_callback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMaskedTextGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_tokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mbert_masked_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_masked_language_bert_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mbert_masked_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-e6a001719c46>\u001b[0m in \u001b[0;36mcreate_masked_language_bert_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[0mencoder_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNUM_LAYERS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mencoder_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbert_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(\n",
      "\u001b[1;32m<ipython-input-42-e6a001719c46>\u001b[0m in \u001b[0;36mbert_module\u001b[1;34m(query, key, value, i)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbert_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Multi headed self-attention\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     attention_output = layers.experimental.preprocessing.MultiHeadAttention(\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mnum_heads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNUM_HEAD\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mkey_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEMBED_DIM\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNUM_HEAD\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.keras.layers.experimental.preprocessing' has no attribute 'MultiHeadAttention'"
     ]
    }
   ],
   "source": [
    "sample_tokens = vectorize_layer([\"I have watched this [mask] and it was awesome\"])\n",
    "generator_callback = MaskedTextGenerator(sample_tokens.numpy())\n",
    "\n",
    "bert_masked_model = create_masked_language_bert_model()\n",
    "bert_masked_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84786761",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Train and Save\n",
    "\"\"\"\n",
    "\n",
    "bert_masked_model.fit(mlm_ds, epochs=5, callbacks=[generator_callback])\n",
    "bert_masked_model.save(\"bert_mlm_imdb.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe56641",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Fine-tune a sentiment classification model\n",
    "\n",
    "We will fine-tune our self-supervised model on a downstream task of sentiment classification.\n",
    "To do this, let's create a classifier by adding a pooling layer and a `Dense` layer on top of the\n",
    "pretrained BERT features.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Load pretrained bert model\n",
    "mlm_model = keras.models.load_model(\n",
    "    \"bert_mlm_imdb.h5\", custom_objects={\"MaskedLanguageModel\": MaskedLanguageModel}\n",
    ")\n",
    "pretrained_bert_model = tf.keras.Model(\n",
    "    mlm_model.input, mlm_model.get_layer(\"encoder_0/ffn_layernormalization\").output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b894e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze it\n",
    "pretrained_bert_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c678f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier_bert_model():\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
    "    sequence_output = pretrained_bert_model(inputs)\n",
    "    pooled_output = layers.GlobalMaxPooling1D()(sequence_output)\n",
    "    hidden_layer = layers.Dense(64, activation=\"relu\")(pooled_output)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "    classifer_model = keras.Model(inputs, outputs, name=\"classification\")\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    classifer_model.compile(\n",
    "        optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return classifer_model\n",
    "\n",
    "\n",
    "classifer_model = create_classifier_bert_model()\n",
    "classifer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be7abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifer_model.fit(\n",
    "    train_classifier_ds, epochs=5, validation_data=test_classifier_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a99949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the BERT model for fine-tuning\n",
    "pretrained_bert_model.trainable = True\n",
    "optimizer = keras.optimizers.Adam()\n",
    "classifer_model.compile(\n",
    "    optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "classifer_model.fit(\n",
    "    train_classifier_ds, epochs=5, validation_data=test_classifier_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e5eaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Create an end-to-end model and evaluate it\n",
    "\n",
    "When you want to deploy a model, it's best if it already includes its preprocessing\n",
    "pipeline, so that you don't have to reimplement the preprocessing logic in your\n",
    "production environment. Let's create an end-to-end model that incorporates\n",
    "the `TextVectorization` layer, and let's evaluate. Our model will accept raw strings\n",
    "as input.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_end_to_end(model):\n",
    "    inputs_string = keras.Input(shape=(1,), dtype=\"string\")\n",
    "    indices = vectorize_layer(inputs_string)\n",
    "    outputs = model(indices)\n",
    "    end_to_end_model = keras.Model(inputs_string, outputs, name=\"end_to_end_model\")\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    end_to_end_model.compile(\n",
    "        optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return end_to_end_model\n",
    "\n",
    "\n",
    "end_to_end_classification_model = get_end_to_end(classifer_model)\n",
    "end_to_end_classification_model.evaluate(test_raw_classifier_ds)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (nlp-updated)",
   "language": "python",
   "name": "nlp2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
